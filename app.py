import streamlit as st
import os
import chromadb
from sentence_transformers import SentenceTransformer
import torch

try:
    from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
    RAG_LLM_READY = True
except Exception:
    RAG_LLM_READY = False
    st.warning("‚ö†Ô∏è Transformers ‡∏´‡∏£‡∏∑‡∏≠ Torch ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏° ‚Äî ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÅ‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏°‡∏µ LLM")

@st.cache_resource
def load_rag_components():
    embed_model = None
    db_collection = None
    rag_pipeline = None

    try:
        embed_model = SentenceTransformer("all-mpnet-base-v2")
    except Exception as e:
        st.error(f"‚ùå ‡πÇ‡∏´‡∏•‡∏î SentenceTransformer ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}")
        return None, None, None

    CHROMA_PATH = "./chroma_db_optimized"
    COLLECTION_NAME = "baroness_orczy_optimized"

    try:
        if not os.path.isdir(CHROMA_PATH):
            st.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå ChromaDB: {CHROMA_PATH}")
            return embed_model, None, None
        
        client = chromadb.PersistentClient(path=CHROMA_PATH)
        db_collection = client.get_collection(name=COLLECTION_NAME)

    except Exception as e:
        st.error(f"‚ùå ‡πÇ‡∏´‡∏•‡∏î ChromaDB ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}")
        return embed_model, None, None

    if not RAG_LLM_READY:
        return embed_model, db_collection, None

    # Qwen2.5 - ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á ‡∏î‡∏µ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Q&A ‡πÅ‡∏•‡∏∞‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
    model_name = "Qwen/Qwen2.5-1.5B-Instruct"

    try:
        with st.spinner(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î LLM {model_name} ..."):

            tokenizer = AutoTokenizer.from_pretrained(model_name)

            # ‡πÉ‡∏ä‡πâ device_map="auto" ‡πÉ‡∏´‡πâ accelerate ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ device ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
            llm_model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                device_map="auto"
            )

            # ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏∏ device ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ device_map="auto"
            rag_pipeline = pipeline(
                "text-generation",
                model=llm_model,
                tokenizer=tokenizer
            )

    except Exception as e:
        st.error(f"‚ùå ‡πÇ‡∏´‡∏•‡∏î LLM ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}")
        return embed_model, db_collection, None

    return embed_model, db_collection, rag_pipeline

embed_model, db_collection, rag_pipeline = load_rag_components()

def get_rag_answer(query_text, embed_model, db_collection, rag_pipeline):
    if not embed_model or not db_collection:
        return "‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥ RAG ‡πÑ‡∏î‡πâ: Embedding/ChromaDB ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°"

    if rag_pipeline is None:
        return "‚ö†Ô∏è Embedding ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ ‡πÅ‡∏ï‡πà LLM ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à"

    try:
        query_embedding = embed_model.encode([query_text])
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô chunks ‡∏ó‡∏µ‡πà retrieve ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ context ‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô
        results = db_collection.query(
            query_embeddings=query_embedding.tolist(),
            n_results=10  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 5 ‡πÄ‡∏õ‡πá‡∏ô 10
        )
        context = "\n\n".join(results["documents"][0])
    except Exception as e:
        return f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ ChromaDB: {e}"

    try:
        prompt = f"""<|im_start|>system
You are a helpful assistant answering questions about the book "The Heart of a Woman" by Baroness Orczy.
Answer based ONLY on the given context. Be thorough and complete.
- For questions asking for lists (e.g., "all characters", "who are"), provide a complete list with brief descriptions.
- For descriptive questions, give detailed answers.
- If the answer is not in the context, say "I don't have that information."
<|im_end|>
<|im_start|>user
Context:
{context}

Question: {query_text}
<|im_end|>
<|im_start|>assistant
"""
        outputs = rag_pipeline(
            prompt,
            max_new_tokens=300,  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 150 ‡πÄ‡∏õ‡πá‡∏ô 300
            temperature=0.3,
            do_sample=True,
            return_full_text=False,
            pad_token_id=rag_pipeline.tokenizer.eos_token_id
        )
        answer = outputs[0]['generated_text'].strip()
        # ‡∏ï‡∏±‡∏î <|im_end|> ‡∏≠‡∏≠‡∏Å‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
        if "<|im_end|>" in answer:
            answer = answer.split("<|im_end|>")[0].strip()
        return answer

    except Exception as e:
        return f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å LLM: {e}"

st.set_page_config(page_title="Book Chat", page_icon="üìö")

if "messages" not in st.session_state:
    msg = "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ! ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÉ‡∏ô‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠ The Heart of a Woman ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢ üòä"
    if rag_pipeline is None:
        msg += "\n\n‚ö†Ô∏è LLM ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÅ‡∏ï‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏à‡∏≤‡∏Å ChromaDB ‡πÑ‡∏î‡πâ"
    else:
        msg += "\n\n‡∏£‡∏∞‡∏ö‡∏ö‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÅ‡∏•‡πâ‡∏ß!"
    st.session_state.messages = [("assistant", msg)]

def inject_css():
    st.markdown("""
<style>
.block-container {padding-top: 80px !important;}
.fixed-header {
    position: fixed;
    top: 0;
    left: 0;
    width: 100%;
    height: 60px;
    background-color: white;
    border-bottom: 1px solid #e0e0e0;
    display: flex;
    align-items: center;
    justify-content: left;
    padding: 0 20px;
    z-index: 9999;
}
.header-title {font-weight: 700; font-size: 22px;}
</style>
""", unsafe_allow_html=True)


def chat_page():
    inject_css()
    st.markdown('<div class="fixed-header"><span class="header-title">üìö Book Chat RAG</span></div>', unsafe_allow_html=True)

    for role, msg in st.session_state.messages:
        with st.chat_message(role):
            st.write(msg)

    prompt = st.chat_input("‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°...")
    if prompt:
        st.session_state.messages.append(("user", prompt))
        with st.chat_message("user"):
            st.write(prompt)

        with st.chat_message("assistant"):
            with st.spinner("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö..."):
                reply = get_rag_answer(prompt, embed_model, db_collection, rag_pipeline)
            st.write(reply)

        st.session_state.messages.append(("assistant", reply))

chat_page()
